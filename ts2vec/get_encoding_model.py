'''
Purpose:
    Train the ts2vec model using the corresponding RNA sequences from the training and testing sets, and save the model parameters.
    The trained model is saved in the `model_file` and then used in the LTP for encoding, and to complete the final downstream training and prediction.

Note:
    When packaging the training set: the feature vectors need to be packaged.
    When packaging the testing set: although the ts2vec module is used, the feature vectors are not packaged.
    The file format required in the specified folder: bpseq or ct file format.
    Used to generate for the specified folder: sequence encoding, label matrix, and feature encoding generated by ts2vec.
    Hyperparameters:
        1: Range of clipping sequence length
        2: Encoding method
    How to use: Only specify the maximum sequence length and RNA family name two parameters each time.

Very important:
    This model needs to train the ts2vec model with the training set and testing set together (only use sequence information, not label information).
    When training the downstream model, it is necessary to distinguish between the training set and the testing set. So, read the training folder and testing folder one by one for easy distinction later on.
'''

'''
 
    The code for encoding the ts2vec model is sourced from: https://github.com/zhihanyue/ts2vec. 
    The difference is that two additional hyperparameters, namely the minimum and maximum lengths for clipping sequences, have been added to the original source code.

'''

import os
import torch
import numpy as np
import sys
sys.path.insert(0, '../util')
from ts2vec import TS2Vec
from utils import get_ct_table, get_data, get_bpseq_table, get_args_from_json, set_random_seed


def encoding_model(dst, limit_length, file_type, name, device, repr_dim, cut_min_len, cut_max_len, epochs):
    

    train_test_file = os.listdir(os.path.join(dst, name))
    family = []
    for i in range(len(train_test_file)):
        family.append(train_test_file[i])

    train_len, test_len, test_len1 = 0, 0, 0
    seq_list = [] 

    for i in range(len(family)):

        path = os.path.join(dst, name, family[i])

        if(file_type == 'bpseq'):
            seq_list_, _, _ = get_bpseq_table(path, limit_length, 0, len(os.listdir(path)))
        elif(file_type == 'ct'):
            seq_list_, _, _ = get_ct_table(path, limit_length, 0, len(os.listdir(path)))
        seq_list.extend(seq_list_)

        if(i == 0):
            train_len = len(seq_list)
        if(i == 1):
            test_len = len(seq_list) - train_len

    seq_list = get_data(np.array(seq_list))

    print('now fitting')
    print(train_len, test_len)

    # Train a TS2Vec model
    model = TS2Vec(
        input_dims=seq_list.shape[-1],
        device=device,
        output_dims=repr_dim
    )

    loss_log = model.fit(
        seq_list,
        min_len=cut_min_len,  
        max_len=cut_max_len, 
        n_epochs=epochs,  
        verbose=True
    )

    model.save('model_file/{}.pth'.format(name))
    print('model save success!')

def main():

    # get args from json file
    args = get_args_from_json('../util/args_Arc128.json')
    
    # torch.manual_seed(args.seed)
    set_random_seed(args.seed)
    
    limit_length = args.max_seq_len
    file_type = args.file_type
    encoding_model_name = args.encoding_model_name
    repr_dim = args.repr_dim
    cut_min_len = args.cut_min_len
    cut_max_len = args.cut_max_len
    epochs = args.encoding_model_epochs
    device = args.device

    name = encoding_model_name + '-{}'.format(limit_length)
    dst = 'datasets'

    encoding_model(dst, limit_length, file_type, name, device, repr_dim, cut_min_len, cut_max_len, epochs)


if __name__ == '__main__':
    
    main()